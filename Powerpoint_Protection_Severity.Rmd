---
title: "Building a Protection Severity Index"
author: "Data Analysis Workshop"
date: "12/2/2019" 
output: 
  powerpoint_presentation:
    reference_doc: templateProt.pptx
    slide_level: 2
---

```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.height = 8, fig.width = 12, dpi = 300, comment = ""  )
# https://support.rstudio.com/hc/en-us/articles/360004672913-Rendering-PowerPoint-Presentations-with-RStudio#structuring-the-presentation  
## https://stackoverflow.com/questions/55598415/r-markdown-powerpoint-slide-customization

# Go into View -> Slide Master to make sure the slides in the master are the format you expect (one time I thought I made the changes, but they showed up on the Home menu and not the Slide Master)
# 
# Under the Home menu, click the Dropdown under Layout and make sure AT THE LEAST that you see these four elements:
# 
#     Title
#     Title and Content
#     Section Header
#     Two Content

### used as follow : 
# Section header (Slide 2)

## Section header (Slide 3)

### Slide Title (Slide 4)
#### Slide content header (Slide 4)
##### Slide content header (Slide 4)

## you can upload your template to https://rmarkdown-office-template.herokuapp.com/ and it will test against these rules 
# https://github.com/sol-eng/powerpoint 

## This may require some xml hacking: 
# http://www.brandwares.com/bestpractices/2015/02/xml-hacking-an-introduction/
# http://www.brandwares.com/bestpractices/2015/08/xml-hacking-table-styles-complete/ 
# http://www.brandwares.com/bestpractices/2015/03/xml-hacking-default-table-text/ 
```

## Building severity Index

Objective of the session:  

 * Building such index, also called __composite indicator__, is among the regular and expected task of any humanitarian data analyst.

 * Objective is simplify information into a simple indicator that can reduce information overflow.

 * Informs the sectoral and inter-sectoral discussions in order the facilitate comparison of needs across geographic areas.

---

## Methodology

When developing severity index, each steps comes with methodological questions: 

 * How to select and organize indicators?   

 * How to calculate and reshape them? 

 * How to assemble them together (aggregation and weighting).

---

```{r setup, include = TRUE, message = FALSE,  echo = FALSE}

## This function will retrieve the packae if they are not yet installed.
using <- function(...) {
    libs <- unlist(list(...))
    req <- unlist(lapply(libs,require,character.only = TRUE))
    need <- libs[req == FALSE]
    if (length(need) > 0) { 
        install.packages(need)
        lapply(need,require,character.only = TRUE)
    }
}

## Getting all necessary package
using("readr","readxl","dplyr","ggplot2","corrplot","psych","bbplot",
      "scales","ggiraph","ggrepel","Compind","ggcorrplot","kableExtra",
      "reshape2","qgraph", "sf","cartography",
      "raster", "xlsx", "rgdal","knitr")

rm(using)



# This small function is used to have nicely left align text within 
# charts produced with ggplot2
left_align <- function(plot_name, pieces){
  grob <- ggplot2::ggplotGrob(plot_name)
  n <- length(pieces)
  grob$layout$l[grob$layout$name %in% pieces] <- 2
  return(grob)
}

unhcr_style <- function() {
  font <- "Lato"
  ggplot2::theme(
    
#This sets the font, size, type and colour of text for the chart's title
  plot.title = ggplot2::element_text(family = font, size = 20,
                                     face = "bold", color = "#222222"),

# This sets the font, size, type and colour of text for the chart's subtitle,
# as well as setting a margin between the title and the subtitle
  plot.subtitle = ggplot2::element_text(family = font, size = 16, 
                                        margin = ggplot2::margin(9,0,9,0)),
  plot.caption = ggplot2::element_blank(),

# This sets the position and alignment of the legend, removes a title 
# and backround for it and sets the requirements for any text within the legend.
# The legend may often need some more manual tweaking when it comes to its exact
# position based on the plot coordinates.
  legend.position = "top",
  legend.text.align = 0,
  legend.background = ggplot2::element_blank(),
  legend.title = ggplot2::element_blank(),
  legend.key = ggplot2::element_blank(),
  legend.text = ggplot2::element_text(family = font, size = 13, color = "#222222"),

# This sets the text font, size and colour for the axis test, as well as setting
# the margins and removes lines and ticks. In some cases, axis lines and axis 
# ticks are things we would want to have in the chart
  axis.title = ggplot2::element_blank(),
  axis.text = ggplot2::element_text(family = font, size = 13, color = "#222222"),
  axis.text.x = ggplot2::element_text(margin = ggplot2::margin(5, b = 10)),
  axis.ticks = ggplot2::element_blank(),
  axis.line = ggplot2::element_blank(),

# This removes all minor gridlines and adds major y gridlines. 
# In many cases you will want to change this to remove y gridlines and add x gridlines. 
  panel.grid.minor = ggplot2::element_blank(),
  panel.grid.major.y = ggplot2::element_line(color = "#cbcbcb"),
  panel.grid.major.x = ggplot2::element_blank(),

# This sets the panel background as blank, removing the standard grey
# ggplot background colour from the plot
  panel.background = ggplot2::element_blank(),

# This sets the panel background for facet-wrapped plots to white, 
# removing the standard grey ggplot background colour and sets the title size 
# of the facet-wrap title to font size 22
  strip.background = ggplot2::element_rect(fill = "white"),
  strip.text = ggplot2::element_text(size  = 13,  hjust = 0)
  )
}

```


## Dataset 

 * Key Informant Interview for Hurricane Dorian, Bahamas  - [available through HDX here](https://data.humdata.org/dataset/the-bahamas-hurricane-dorian-site-assessment-round-3-november-2019#)

 * Collected at the beginning of November 2019 by IOM/DTM in the Bahamas. See [initial report](https://displacement.iom.int/reports/bahamas-%E2%80%94-hurricane-dorian-%E2%80%93-site-assessment-%E2%80%93-round-3-november-2019?close=true). 

 * Covers Abaco Island which has been severely hit by Hurricane Dorian on 1-3 September 2019.

```{r,  echo = FALSE, warning = FALSE, message = FALSE}
data <- read_excel("DTM R3 DB Great-Little Abaco MSLA V4.xlsx", sheet = "BD")


## For some field the value '9999' has been entered - for the sake of analysis, 
# we will assume that this correspond to answers 'no' - 0
# to replace multiple values in a data frame, looping through all columns might help.
for (i in seq_along(data)) {
    data[[i]][data[[i]] == "9999"] <- "0"
}
# This has converted numerci to character - let's bring them back to numeric
data[, c(3:ncol(data))] <- sapply(data[, c(3:ncol(data))], as.numeric)

## Point of control
#View(data[ ,c("J_107_VisitInmigration")])

```

---

## Theoretical Severity Framework

```{r echo=FALSE, message=FALSE, warning=FALSE}
## Need to implemet manually the analysis framework in the data dictionnary in order 
# to get the calculation
dico <- read_excel("DTM R3 DB Great-Little Abaco MSLA V4.xlsx", sheet = "CatPregs")


## Checking the dico matche with dataframe --
dico1 <- as.data.frame(names(data))
names(dico1)[1] <- "cod_pregunta"
dico <- plyr::join(x = dico1 , y = dico , by = "cod_pregunta", type = "left" )
# Adding variable label in data frame
for (i in 1:nrow(data)) { attributes(data)$variable.labels[ i] <- as.character(dico[ i, c("label")]) }


rm(i, dico1)

## Now creating score variable and applying same direction to all of them!

#levels(as.factor(dico$Calculation))
#labels(dico)
subindicator <- dico[ dico$Calculation %in% c("binary", "value", "scored"), ]
subindicator.unique <- as.data.frame( unique(subindicator[ ,c("qid",  "question",
                                                              "qtype" ,
                                                              "Calculation", "Dimension",
                                                              "qlabel","justification",
                                                              "Polarity"   )]))

## Display the table
subindicator.unique2 <- as.data.frame( unique(subindicator[ ,c( "Dimension","qlabel",
                                                                "qtype" , "Calculation",
                                                                "Polarity"   )]))
row.names(subindicator.unique2) <- NULL

##  Frame with all dimensions
dimensions <- as.data.frame( unique(subindicator[ ,c( "Dimension" )]))
names(dimensions)[1] <- "Dimension"
## Get first Protection
i <- 2
  ## looping around dimensions
this.dimension <- as.character(dimensions[i,1])
  
subindicator.unique3 <- as.data.frame( subindicator.unique2[ subindicator.unique2$Dimension == this.dimension ,
                                                  c( "qlabel",  "Calculation",
                                                                "Polarity"   )])
row.names(subindicator.unique3) <- NULL  

knitr::kable(subindicator.unique3, format = 'markdown')

#subindicator.unique3
```


## Calculating sub-indicator

Different calculations were used depending on the type of questions used in the severity framework:

 * For `binary` questions, negative questions receive a score of 1 when answered positively, and 0 when  “no”
 
 * Some `select_one` response choices is ordinal data with an imputed ordered response, where the max score is given either to the best or worst possible choice. 
 
 * Some `select_multiple` questions might have many dummy variables corresponded to the question and the sum of these scores represented the highest score possible.  Other `select_multiple` response choices are discrete choice data with nominal responses; each answer in the `select_multiple` are weighted according their severity.
 

---

```{r message=FALSE, warning=FALSE,  echo = FALSE}
## Creating scores for each of those indicator ###########
## Num col where indic will start to be appended
numcol1 <- ncol(data) + 1
numcol <- ncol(data)

## looping around each new sub indicator to create
for (j in 1:nrow(subindicator.unique)) {

    # j <- 2
    this.indicator.comp <- as.character(subindicator.unique[ j, c("Calculation")])
    this.indicator.type <- as.character(subindicator.unique[ j, c("qtype")])
    this.indicator.name <- as.character(subindicator.unique[ j, c("qid")])
    this.indicator.label <- as.character(subindicator.unique[ j, c("qlabel")])
    this.indicator.question <- as.character(subindicator.unique[ j, c("question")])
    this.indicator.Polarity <- as.character(subindicator.unique[ j, c("Polarity")])
    this.indicator.Dimension <- as.character(subindicator.unique[ j, c("Dimension")])

    ## let's add a variable in the data frame and give it the indic name
    numcol <- numcol + 1
    data[ , numcol] <- ""
    names(data)[numcol] <- this.indicator.name
    attributes(data)$variable.labels[numcol] <- this.indicator.label

    ## Display where we are in the console - always usefull to debug!
    ## Take the # when playing with the script
    #cat(paste0("\n\n===  Indicator: ", j, "-",this.indicator.Dimension ,
    #           "-",this.indicator.label ,"\n",this.indicator.comp ,
    #           "-",this.indicator.Polarity ,"\n"))

    ## Now accounting for 2 distinct cases to get my calculation
    ## "binary" / "value" or "scored"

      ## If it's a score, we need to sum up all scores for that questions  #####
      # - independently of wether it's a select_one or select_multiple -
      if (this.indicator.comp == "scored" ) {
      ## get the correponding value var
        this.value.var <- as.character(subindicator[ subindicator$question == this.indicator.question,
                                                     c("cod_pregunta") ])
        this.subset <- data[ , this.value.var]

        ## Now get an apply the coeffcient
        this.subsetscore <- t(as.data.frame(subindicator[ subindicator$cod_pregunta %in% this.value.var,
                                                          c("scoremodality") ]))

      ## multiply data frame by a vector
      this.subset3 <- data.frame(mapply(`*`,this.subset, this.subsetscore, SIMPLIFY = FALSE))
      #str(this.subset3)
      # this.subset4 <- cbind(this.subset3,  rowSums(this.subset3, na.rm = TRUE))

      #cat(paste0("Calculation of Indicator: ", j, "\n"))
      ## Get the sum of the row
      data[ , numcol] <- rowSums(this.subset3, na.rm = TRUE)
    }
     

    ## If it's a  value, we just take the value of that binary  #####-
    if (this.indicator.comp %in% c("value")) {
      ## get the correponding value var
      this.value.var <- as.character(dico[ dico$question == this.indicator.question,
                                           c("cod_pregunta") ])
      ## Apply the value
      #cat(paste0("Calculation of Indicator: ", j, "\n"))
      data[ , numcol] <- data[ , this.value.var]
    }
    
    ## If it's a binary , we add 1 to avoid zero value #####-
    if (this.indicator.comp %in% c("binary")) {
      ## get the correponding value var
      this.value.var <- as.character(dico[ dico$question == this.indicator.question,
                                           c("cod_pregunta") ])
      ## Apply the value
      #cat(paste0("Calculation of Indicator: ", j, "\n"))
      data[ , numcol] <- data[ , this.value.var] + 1
    }
      
    ## clean
    rm(this.indicator.comp, this.indicator.type, this.indicator.name,
       this.indicator.label, this.indicator.question, this.indicator.Polarity ,
       this.indicator.Dimension, this.subset, this.subset3, this.subsetscore, this.value.var )
}


```


##  Sub-Indicator Polarity

The __polarity__ of a sub-indicator is the sign of the relationship between the indicator and the phenomenon to be measured (_for instance, in a well-being index, "GDP per capita" has 'positive' polarity and "Unemployment rate" has 'negative' polarity_). 

In this case, we have 2 options for such directional adjustments:  

- Negative (the higher score, the more severe)
- Positive (the higher score, the less severe)
 
--- 

##  Data Normalization

Allows for __adjustments of distribution__ (similar range of variation) and scale (common scale) of sub-indicators that may reflect different units of measurement and different ranges of variation. Different possible options:

 * A z-score approach:  Imposes a distribution with mean zero and variance 1. Standardized scores which are below average will become negative, implying that further geometric aggregation will be prevented.
 
 * A min-max approach: Same range of variation [0,1]) but not same variance. This method is very sensitive to extreme values/outliers
 
 * A ranking method: Scores are replaced by ranks – e.g. the highest score receives the first ranking position (rank 1).

---


```{r, message=FALSE, warning=FALSE, echo = FALSE}
## Double Checking results
#View(data[ , numcol1:ncol(data)])
indic <- data[ , numcol1:ncol(data)]

## Remove var when standard deviation is 0
indic2 <- indic[, sapply(indic, function(x) { sd(x) != 0} )]

## For some field, we still have indic with zero value - 
## This a data quality issue that shows that some modalities were missing 
# for select_one or select_multiple
## We assume that this account for not severe situation - i.e. replaced by one
for (i in seq_along(indic2)) {
    indic2[[i]][indic2[[i]] == "0"] <- "1"
}
# This has converted numerci to character - let's bring them back to numeric
indic2 <- sapply(indic2, as.integer)

## Refresh my dictionnary of indic
subindicator.unique2 <- subindicator.unique[ subindicator.unique$qid
                                             %in% names(as.data.frame(indic2)), ]

## Transform this object as a matrix and inject location name as row.names
indic.matrix <- as.matrix(indic2)
row.names(indic.matrix) <- data$C_101_name


## Retrieve polarity from dictionnary
for (i in 1:nrow(subindicator.unique2)) {
  if (subindicator.unique2[ i, c("Polarity")] == "Negative (the higher score, the more severe)")  
    {subindicator.unique2[ i, c("polarity")]  <- "NEG"} else 
    {subindicator.unique2[ i, c("polarity")]  <- "POS"}
 }
subindicator.unique2$dir <- 1

## Case 1
subindicator.scored <- subindicator.unique2[ subindicator.unique2$Calculation == "scored", ]
var.scored <- as.character(subindicator.scored[ , c("qid") ])
indic.matrix.scored <- indic.matrix[ , var.scored ]
indic.matrix.scored.obj <- normalise_ci(indic.matrix.scored,
                                     c(1:ncol(indic.matrix.scored)),
                                     polarity =  as.character(subindicator.scored$polarity ),
                                     method = 3)

## Case 2
# subindicator.value <- subindicator.unique2[ subindicator.unique2$Calculation == "value", ]
# var.value <- as.character(subindicator.value[ , c("qid") ])
# indic.matrix.value <- indic.matrix[ , var.value ]
# indic.matrix.value.obj <- normalise_ci(indic.matrix.value,
#                                      c(1:ncol(indic.matrix.value)),
#                                      polarity =  as.character(subindicator.value$polarity ),
#                                      method = 2)

## Case 3
subindicator.binary <- subindicator.unique2[ subindicator.unique2$Calculation == "binary", ]
var.binary <- as.character(subindicator.binary[ , c("qid") ])
indic.matrix.binary <- indic.matrix[ , var.binary ]
indic.matrix.binary.obj <- normalise_ci(indic.matrix.binary,
                                     c(1:ncol(indic.matrix.binary)),
                                     polarity =  as.character(subindicator.binary$polarity ),
                                     method = 3)

## Binding this together so that we have the full normalised matrix
indic.matrix.norm <- cbind(indic.matrix.scored.obj$ci_norm,
                          # indic.matrix.value.obj$ci_norm,
                           indic.matrix.binary.obj$ci_norm)

## Clean the work environment  
rm( subindicator.unique,
 #  subindicator.value, var.value , indic.matrix.value, indic.matrix.value.obj,
   subindicator.binary, var.binary , indic.matrix.binary, indic.matrix.binary.obj,
   subindicator.scored, var.scored , indic.matrix.scored, indic.matrix.scored.obj   )

```

---

## Correlation Analysis

```{r, message=FALSE, warning=FALSE, echo = FALSE}


##  Frame with all dimensions
dimensions <- as.data.frame( unique(subindicator[ ,c( "Dimension" )]))
names(dimensions)[1] <- "Dimension"

## Creating severity subindice on each dimensions with Data Envelopment analysis #####

#for (i in 1:nrow(dimensions)) {
#for (i in 1:2) {  
  i <- 2
  ## looping around dimensions
  this.dimension <- as.character(dimensions[i,1])
  ## subset related indicator names
  this.indicators <- as.character(subindicator.unique2[ subindicator.unique2$Dimension == this.dimension,
                                                        c("qid") ])
  this.indicators.label <- as.character(subindicator.unique2[ subindicator.unique2$Dimension == this.dimension,
                                                        c("qlabel") ])
  ##subset matrix & df
  this.indic.matrix.norm <- indic.matrix[ , this.indicators]
  this.indic.df <- indic2[ , this.indicators]

## Check correlation
corr.matrix <- cor(this.indic.matrix.norm, method = "pearson",  use = "pairwise.complete.obs")
  
  
## replace with Label inside the matrix
corr.matrix1 <- corr.matrix
rownames(corr.matrix1) <- as.character(subindicator.unique2[subindicator.unique2$qid %in% rownames(corr.matrix), c("qlabel")])
colnames(corr.matrix1) <- as.character(subindicator.unique2[subindicator.unique2$qid %in% colnames(corr.matrix), c("qlabel")])

plot1 <- ggcorrplot(corr.matrix1 ,
                    method = "circle",
                    hc.order = TRUE,
                    type = "upper") +
  labs(title = paste0( "Severity Indicators for ",this.dimension ),
       subtitle = "Identified Correlation between indicators",
       caption = "Correlation level = dot size, Positive Correlation  = Red - Negative = Blue",
       x = NULL, y = NULL) +
  bbc_style() +
  theme( plot.title = element_text(size = 13),
         plot.subtitle = element_text(size = 11),
         plot.caption = element_text(size = 7, hjust = 1),
         axis.text = element_text(size = 7),
         strip.text.x = element_text(size = 7),
         axis.text.x = element_text(angle = 45, hjust = 1),
         legend.position = "top",
         legend.box = "horizontal",
         legend.text = element_text(size = 9),
         panel.grid.major.x = element_line(color = "#cbcbcb"),
         panel.grid.major.y = element_line(color = "#cbcbcb"))
ggpubr::ggarrange(left_align(plot1, c("subtitle", "title")), ncol = 1, nrow = 1)

```



---

## Correlations Network 

```{r, message=FALSE, warning=FALSE, echo = FALSE}

qgraph(cor(this.indic.matrix.norm),
     # shape = "circle",
     # posCol = "darkgreen",
     # negCol = "darkred",
     # threshold = "bonferroni", #The threshold argument can be used to remove edges that are not significant.
     # sampleSize = nrow(scores.this.norm),
     # graph = "glasso",
       esize = 35, ## Size of node
       vsize = 6,
       vTrans = 600,
       posCol = "#003399", ## Color positive correlation Dark powder blue
       negCol = "#FF9933", ## Color negative correlation Deep Saffron
       alpha = 0.05,
       cut = 0.4, ## cut off value for correlation
       maximum = 1, ## cut off value for correlation
       palette = 'pastel', # adjusting colors
       borders = TRUE,
       details = FALSE,
       layout = "spring",
       nodeNames = this.indicators.label ,
       legend.cex = 0.4,
       title = paste0("Correlations Network for severity indicators related ",this.dimension ),
       line = -2,
       cex.main = 3)
```

---

## Consistency 

Cronbach’s alpha, (or coefficient alpha), measures reliability / consistency (i.e.  how well a test measures what it should: measure of the stability of test scores). 

As a rule of thumbs, a score of more than 0.7 indicates an acceptable level of consistency:
- A high level for alpha may mean that all indicators are highly correlated (meaning we have redundant indicators representing the same thing...). 
- A low value for alpha may mean that there are not enough indicators or that the indicators are poorly interrelated.


```{r message=FALSE, warning=FALSE, comment=NA, echo = FALSE}
Cronbach.this <- psych::alpha(this.indic.matrix.norm, check.keys = TRUE)

cat(paste0("The Cronbach Alpha measure of consistency for this combination of indicators is  ", round(Cronbach.this$total$std.alpha, 2), "\n." ) )
```

---

## Aggregation & Weighting

For weighting, the main issue to address is related to the concept of **compensability**. Namely the question is to know to what extent can we accept that the high score of an indicator go to compensate the low score of another indicator? 

This problem of compensability is intertwined with the issue of attribution of weights for each sub-indicator in order to calculate the final aggregation. 

Simply using _"equal weight"_ (all indicators account for the same in the final index) and _"arithmetic aggregation"_ (all indicators are substituable) is unlikely to depict the complex issue of Humanitarian Severity and is likely to comes with the risk of misrepresenting the reality. 

---

## Benefice of the Doubt

- Benefit of the Doubt approach (BoD), application of Data Envelopment Analysis (DEA), ensures that weights are endogenously determined by the observed performances. Benchmark is not based on theoretical bounds, but it’s a linear combination of the observed best performances.

- Directional Benefit of the Doubt (D-BoD) model enhances non-compensatory property by introducing directional penalties in a standard BoD model in order to consider the preference structure among simple indicators. 

- Robust Benefit of the Doubt approach (RBoD) is an extension of BoD based on the concept of the expected minimum input function.

- Benefit of the Doubt approach (BoD) index with constraints on weights allows for constraints (so that there is none not valued) and with a penalty.

---

## Other algorithms

- Factor analysis groups together simple indicators to estimate a composite indicator that captures as much as possible of the information common to individual indicators.

- Mean-Min Function (MMF) is an intermediate case between arithmetic mean, according to which no unbalance is penalized, and min function, according to which the penalization is maximum. 

- Geometric aggregation uses the geometric mean to aggregate the single indicators and therefore allows to bypass the full compensability hypothesis using geometric mean. 

- Mazziotta-Pareto Index (MPI) transforms a set of individual indicators in standardized variables and summarizes them using an arithmetic mean adjusted by a "penalty" coefficient related to the variability of each unit.

- Wroclaw taxonomy method is based on the distance from a theoretical unit characterized by the best performance for all indicators considered.



```{r, message=FALSE, warning=FALSE, echo = FALSE, comment= ""}

CI_BoD_estimated = ci_bod(this.indic.matrix.norm,
                          indic_col = (1:ncol(this.indic.matrix.norm)))

ci_bod_est <- as.data.frame( CI_BoD_estimated$ci_bod_est)
names(ci_bod_est) <- "Benef_Doubt"

## Endogenous weight - no zero weight --
CI_Direct_BoD_estimated <-  ci_bod_dir(this.indic.matrix.norm,
                                       indic_col = (1:ncol(this.indic.matrix.norm)),
                                       dir = as.numeric(subindicator.unique2[subindicator.unique2$Dimension == this.dimension , c("dir")] ))

ci_bod_dir_est <- data.frame(CI_Direct_BoD_estimated$ci_bod_dir_est)
names(ci_bod_dir_est) <- "Benef_Doubt_Dir"

CI_RBoD_estimated <-  ci_rbod(this.indic.matrix.norm,
                              indic_col = (1:ncol(this.indic.matrix.norm)),
                              M = 20,  #The number of elements in each sample.
                              B = 200) #The number of bootstap replicates.

ci_rbod_est <- data.frame(CI_RBoD_estimated$ci_rbod_est)
names(ci_rbod_est) <- "Benef_Doubt_Rob"


CI_BoD_MPI_estimated = ci_bod_constr_mpi(this.indic.matrix.norm,
                                          indic_col = (1:ncol(this.indic.matrix.norm)),
                                          up_w = 1,
                                          low_w = 0.1,
                                          penalty = "POS")

ci_bod_constr_est_mpi <- data.frame(CI_BoD_MPI_estimated$ci_bod_constr_est_mpi)
names(ci_bod_constr_est_mpi) <- "Benef_Doubt_Cons"

##  Doing PCA with ci_factor.R
# If method = "ONE" (default) the composite indicator estimated values are equal to first component scores;
# if method = "ALL" the composite indicator estimated values are equal to component score multiplied by its proportion variance;
# if method = "CH" it can be choose the number of the component to take into account.
dimfactor <- ifelse(ncol(this.indic.matrix.norm) > 2, 3, ncol(this.indic.matrix.norm))
CI_Factor_estimated <-  ci_factor(this.indic.matrix.norm,
                                  indic_col = (1:ncol(this.indic.matrix.norm)),
                                  method = "CH",  # if method = "CH" it can be choose the number of the component to take into account.
                                  dim = dimfactor)
ci_factor_est <- data.frame( CI_Factor_estimated$ci_factor_est)
names(ci_factor_est) <- "Factor"

CI_mean_min_estimated <- ci_mean_min(this.indic.matrix.norm,
                                     indic_col = (1:ncol(this.indic.matrix.norm)),
                                     alpha = 0.5,  #intensity of penalization of unbalance  (alpha)
                                     beta = 1) # intensity of complementarity (beta) among indicators

ci_mean_min_est <- data.frame( CI_mean_min_estimated$ci_mean_min_est)
names(ci_mean_min_est) <- "Mean_Min"


CI_Geom_estimated = ci_geom_gen(this.indic.matrix.norm,
                                indic_col = (1:ncol(this.indic.matrix.norm)),
                                meth = "EQUAL",
                                ## "EQUAL" = Equal weighting set, "BOD" = Benefit-of-the-Doubt weighting set.
                                up_w = 1,
                                low_w = 0.1,
                                bench = 1)
# Row number of the benchmark unit used to normalize the data.frame x.

ci_mean_geom_est <- data.frame( CI_Geom_estimated$ci_mean_geom_est)
names(ci_mean_geom_est) <- "Mean_Geom"

CI_MPI_estimated <- ci_mpi(this.indic.matrix.norm,
                           indic_col = (1:ncol(this.indic.matrix.norm)),
                           penalty = "NEG")  # Penalty direction; ”POS” (default) in case of increasing
#  or “positive” composite index (e.g., well-being index),
#  ”NEG” in case of decreasing or “negative” composite
#  index (e.g., poverty index).

ci_mpi_est <- data.frame( CI_MPI_estimated$ci_mpi_est)
names(ci_mpi_est) <- "Mazziotta_Pareto"


CI_wroclaw_estimated <-  ci_wroclaw(this.indic.matrix.norm,
                                    indic_col = (1:ncol(this.indic.matrix.norm)))

ci_wroclaw_est <- data.frame( CI_wroclaw_estimated$ci_wroclaw_est)
names(ci_wroclaw_est) <- "Wroclaw"

```

---

```{r, message=FALSE, warning=FALSE, echo = FALSE, results = 'hide'}
this.indic.matrix.norm2 <- cbind( #row.names(scores.this),
  ci_bod_est, # Benefit of the Doubt approach
  ci_rbod_est, # Robust Benefit of the Doubt approach
  ci_bod_dir_est, # Directional Robust Benefit of the Doubt approach
  ci_bod_constr_est_mpi, # Robust Benefit of the Doubt approach with constraint
  ci_factor_est, # Factor analysis  componnents
  ci_mean_geom_est, # Geometric aggregation
  ci_mean_min_est, # Mean-Min Function
  ci_mpi_est, # Mazziotta-Pareto Index
  ci_wroclaw_est) # Wroclaw taxonomy method


# knitr::kable(this.indic.matrix.norm2, caption = "Composite with different algorithm") %>%
#            kable_styling(bootstrap_options = c("striped", "bordered", "condensed", "responsive"), font_size = 9)

## Eliminate automatically method that could not score some elements
this.indic.matrix.norm22 <- this.indic.matrix.norm2[, colSums(this.indic.matrix.norm2 != 0, na.rm = TRUE) > 0]

## Check if sum is  zero
this.indic.matrix.norm22 <- this.indic.matrix.norm22[, colSums(this.indic.matrix.norm22 != 0, na.rm = TRUE)  == nrow(this.indic.matrix.norm22)]

## Remove indic if standard deviation is o
this.indic.matrix.norm22 <- this.indic.matrix.norm22[, sapply(this.indic.matrix.norm22, function(x) { sd(x) != 0} )]

## Remove “NaN” or “Not a Number
this.indic.matrix.norm22 %>%
  summarise_all(function(x) sum(x[!is.na(x)] == "NaN") == length(x[!is.na(x)])) %>% # check if number of NaN is equal to number of rows after removing NAs
  select_if(function(x) x == FALSE) %>%       # select columns that don't have only nulls
  names() -> vars_to_keep                     # keep column names

this.indic.matrix.norm22 %>% dplyr::select(vars_to_keep)                  # select columns captured above



rm( ci_bod_constr_est_mpi, # Robust Benefit of the Doubt approach with constraint
  CI_BoD_MPI_estimated,
  ci_bod_est, # Benefit of the Doubt approach
  CI_BoD_estimated,
  ci_bod_dir_est, # Directional Robust Benefit of the Doubt approach
  CI_Direct_BoD_estimated,
  ci_rbod_est, # Robust Benefit of the Doubt approach
  CI_RBoD_estimated,
  ci_factor_est, # Factor analysis  componnents,
  dimfactor,
  CI_Factor_estimated,
  ci_mean_min_est, # Mean-Min Function
  CI_mean_min_estimated,
  ci_mpi_est, # Mazziotta-Pareto Index
  CI_MPI_estimated,
  ci_mean_geom_est, # Geometric aggregation
  CI_Geom_estimated,
  ci_wroclaw_est,# Wroclaw taxonomy method
  CI_wroclaw_estimated) 


## Factor analysis can provide negative rank - If it works we need to get rid of them
for (j in 1:nrow(this.indic.matrix.norm22)) {
this.indic.matrix.norm22[j ,c("Factor")] <- ifelse( min(this.indic.matrix.norm22[ ,c("Factor")]) < 0 ,
                                                     this.indic.matrix.norm22[j ,c("Factor")] + abs(min(this.indic.matrix.norm22[ ,c("Factor")])) ,
                                                     this.indic.matrix.norm22[j ,c("Factor")]) 
}

kept.methodo <- as.data.frame(names(this.indic.matrix.norm22))
kept.methodo$polarity <- "POS"
polarity2 <- as.character(kept.methodo$polarity)


this.indic.matrix.norm3 <- normalise_ci(this.indic.matrix.norm22,
                                        c(1:ncol(this.indic.matrix.norm22)),
                                        polarity =  polarity2,
                                        method = 3)
this.indic.matrix.norm3 <- this.indic.matrix.norm3$ci_norm

knitr::kable(this.indic.matrix.norm3, caption = "Location Ranking with different algorithms") 

```

---

## Compare Output

```{r, message=FALSE, warning=FALSE, echo = FALSE}

## Remove NaN
this.indic.matrix.norm4 <-  this.indic.matrix.norm3[,colSums(this.indic.matrix.norm3 != 0, na.rm = TRUE) > 0]
## Add blank variable for nice chart display
this.indic.matrix.norm4$Location <- NA
  
this.indic.matrix.norm4.melt <- melt(as.matrix(this.indic.matrix.norm4))


#Make plot
line <- ggplot(this.indic.matrix.norm4.melt, aes(x = Var2,
                                                 y = value,
                                                 color = Var1,
                                                 group = Var1)) +
  geom_line(size = 2) +
  scale_colour_manual(values = c("#8dd3c7","#A6CEE3", "#1F78B4", "#B2DF8A", "#33A02C",
                                 "#FB9A99", "#E31A1C", "#FDBF6F", "#FF7F00", "#CAB2D6",
                                 "#6A3D9A", "#fb8072", "#B15928", "#fdb462","#ccebc5",
                                 "#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442",
                                 "#0072B2", "#D55E00", "#CC79A7")) + ## as many color as locations.. 23
  geom_text_repel(
    data = this.indic.matrix.norm4.melt[ this.indic.matrix.norm4.melt$Var2 == "Wroclaw", ],
    aes(label = Var1),
    arrow = arrow(length = unit(0.03, "npc"), type = "closed", ends = "first"),
    direction =	"y",
    size = 4,
    nudge_x = 45 ) +
  labs(title = paste0("Rank for Composite Indicator on ",  this.dimension ),
       subtitle = "Based on various weighting approach") +
  bbc_style() +
  theme( plot.title = element_text(size = 13),
         plot.subtitle = element_text(size = 11),
         plot.caption = element_text(size = 7, hjust = 1),
         axis.text = element_text(size = 10),
         strip.text.x = element_text(size = 11),
         panel.grid.major.x = element_line(color = "#cbcbcb"),
         panel.grid.major.y = element_blank(),
         axis.text.x = element_text(angle = 45, hjust = 1),
         legend.position = "none")

print(ggpubr::ggarrange(left_align(line, c("subtitle", "title")), ncol = 1, nrow = 1))

```



```{r, message=FALSE, echo = FALSE, warning=FALSE, results = 'hide'}


## For mapping we normalise on mena-max
var <- c("Benef_Doubt", 
           "Benef_Doubt_Rob", 
           "Benef_Doubt_Dir",
           "Benef_Doubt_Cons",
           "Factor",
           "Mean_Geom",       
           "Mean_Min",
           "Mazziotta_Pareto",
           "Wroclaw" )

label <- c( "Benefit of the Doubt approach",
               "Robust Benefit of the Doubt approach",
               "Directional Robust Benefit of the Doubt approach",
               "Robust Benefit of the Doubt approach with constraint",
               "Factor analysis  componnents",
               "Geometric aggregation",
               "Mean-Min Function",
               "Mazziotta-Pareto Index",
               "Wroclaw taxonomy method")
polarity <- c( "POS",
                "POS",
                "POS",
                "POS",
                "POS",
                "POS",
                "POS",
                "POS",
                "NEG")

All.method <- as.data.frame( cbind(var, label,polarity))

## Filter method that are kept
All.method2 <- All.method[All.method$var %in% names(this.indic.matrix.norm22), ]

## Filter method that are kept
All.method2 <- All.method[All.method$var %in% names(this.indic.matrix.norm22), ]
polarity2 <- as.character(All.method2[ , c("polarity")])

this.indic.matrix.norm3 <- normalise_ci(this.indic.matrix.norm22,
                                        c(1:ncol(this.indic.matrix.norm22)),
                                        polarity =  polarity2,
                                        method = 2)
this.indic.matrix.norm3 <- this.indic.matrix.norm3$ci_norm * 100

## Creating a spatial Point data frame with coordinates
abaco <- SpatialPointsDataFrame(data[ ,c("C_103_lon","C_102_lat")], ## Coord
                                cbind(as.data.frame(this.indic.matrix.norm3),data[ ,c("D_101_fams","D_102_inds","C_101_name")] ), ## Composite with different algo - together with population
                                proj4string = CRS("+init=epsg:4326") ) ## Define projection syst 

## Create equivalent polygo with voronoi
# Using the union of points instead of the original sf object works:
voronoi <- st_voronoi(st_union(st_as_sf(abaco)))
# Clid boundary instead of the original island boundaries


abacomask <- readOGR( paste0(getwd(),"/abaco.geojson"), verbose = FALSE)
abacopoly <- st_intersection(st_union(st_as_sf(abacomask)),
                             st_cast(voronoi))
abacopoly2 <- as(abacopoly, "Spatial")

rownames(abaco@data) <- NULL
abaco@data$ID <- rownames(abaco@data)
abacodata <- as.data.frame(abaco@data)
rownames(abacodata) <- paste0("ID",abaco$ID)
abacopoly3 <- SpatialPolygonsDataFrame(abacopoly2, abacodata)



#plot(abaco)
# Create a boundary box

## https://www.openstreetmap.org/export#map=9/26.4226/-77.3259
## 27.1007 / 25.7232 -- -76.6516 -- -77.8683  
longitudes <- c(-78.3, -76.3)
latitudes <- c(27.09, 25.75)
bounding_box <- SpatialPointsDataFrame( as.data.frame(cbind(longitudes, latitudes)),
                                        as.data.frame(cbind(longitudes, latitudes)) , 
                                        proj4string = CRS("+init=epsg:4326") )

# download osm tiles
abaco.osm <- getTiles(
   x = bounding_box,
   type = "osm",
   zoom = 11,
   crop = TRUE,
   verbose = FALSE
  )

#tilesLayer(x = abaco.osm)

```

---

#  Robust Benefit of the Doubt approach

## Severity Index Map


```{r, message=FALSE, echo = FALSE, warning=FALSE}

#for (h in 1:nrow(All.method2)) {
  h <- 2
  this.var <- as.character(All.method2[ h, c("var")])
  this.label <- as.character(All.method2[ h, c("label")])
  # get Map Background
  tilesLayer(x = abaco.osm)
  #Plot symbols with choropleth coloration
  propSymbolsChoroLayer(
    spdf = abaco,
    var = "D_102_inds",
    inches = 0.25, ## size of the biggest symbol (radius for circles, width for squares, height for bars) in inches.
    border = "grey50",
    lwd = 1, #width of symbols borders.
    legend.var.pos = "topright",
    legend.var.title.txt = "Population Size\n(# of Individual)",
    var2 = this.var ,
    #classification method; one of "sd", "equal", "quantile", "fisher-jenks","q6", "geom", "arith", "em" or "msd" 
    method = "quantile",  
    nclass = 5,
    col = carto.pal(pal1 = "sand.pal", n1 = 6),
    #legend.var2.values.rnd = -2,
    legend.var2.pos = "left",
    legend.var2.title.txt = "Index Value\n",
    legend.var.style = "e"
  )
  # Layout
  layoutLayer(title = paste0("Protection Index - based on ",this.label ),
              author = "Protection Working Group, Abaco - The Bahamas, November 2019",
              sources = "Source: Key Informant Interview - HDX/DTM/IOM", 
              tabtitle = TRUE, 
              frame = FALSE ,
              bg = "#aad3df",
              scale = "auto")
  # North arrow
  north(pos = "topleft")
  
```

---

## Now getting smooth map

```{r, message=FALSE, echo = FALSE, warning=FALSE}  
  
smoothLayer(
  spdf = abacopoly3,
  var = this.var ,
  ###  "pareto" (means power law) or "exponential"
  typefct = "pareto",
  # distance where the density of probability of the spatial interaction function equals 0.5.
  span = 5000,
  # impedance factor for the spatial interaction function.
  beta = 8, 
  nclass = 5,
  col = carto.pal(pal1 = 'brown.pal', n1 = 6),
  border = "grey",
  lwd = 0.1,
  mask = abacopoly,
  #legend.values.rnd = -3,
  legend.title.txt = "Severity\nIndex",
  legend.pos = "topright"
)
# annotation on the map
text(x = 692582, y = 1611478, cex = 0.8, adj = 0, font = 3,  labels =
       "Distance function:\n- type = pareto\n- beta = 8\n- span = 5 km")
# Layout
layoutLayer(title = paste0("Protection Index Spread - based on ",this.label ),
            author = "Protection Working Group, Abaco - The Bahamas, November 2019",
            sources = "Source: Key Informant Interview - HDX/DTM/IOM",
            tabtitle = TRUE,
            frame = FALSE ,
            bg = "#aad3df",
            scale = "auto")
# North arrow
north(pos = "topleft")
  


```



# Factor analysis  componnents

## Severity Index Map


```{r, message=FALSE, echo = FALSE, warning=FALSE}

#for (h in 1:nrow(All.method2)) {
  h <- 4
  this.var <- as.character(All.method2[ h, c("var")])
  this.label <- as.character(All.method2[ h, c("label")])
  # get Map Background
  tilesLayer(x = abaco.osm)
  #Plot symbols with choropleth coloration
  propSymbolsChoroLayer(
    spdf = abaco,
    var = "D_102_inds",
    inches = 0.25, ## size of the biggest symbol (radius for circles, width for squares, height for bars) in inches.
    border = "grey50",
    lwd = 1, #width of symbols borders.
    legend.var.pos = "topright",
    legend.var.title.txt = "Population Size\n(# of Individual)",
    var2 = this.var ,
    #classification method; one of "sd", "equal", "quantile", "fisher-jenks","q6", "geom", "arith", "em" or "msd" 
    method = "quantile",  
    nclass = 5,
    col = carto.pal(pal1 = "sand.pal", n1 = 6),
    #legend.var2.values.rnd = -2,
    legend.var2.pos = "left",
    legend.var2.title.txt = "Index Value\n",
    legend.var.style = "e"
  )
  # Layout
  layoutLayer(title = paste0("Protection Index - based on ",this.label ),
              author = "Protection Working Group, Abaco - The Bahamas, November 2019",
              sources = "Source: Key Informant Interview - HDX/DTM/IOM", 
              tabtitle = TRUE, 
              frame = FALSE ,
              bg = "#aad3df",
              scale = "auto")
  # North arrow
  north(pos = "topleft")
  
```


---


## Now getting smooth map

```{r, message=FALSE, echo = FALSE, warning=FALSE}  
  
 smoothLayer(
  spdf = abacopoly3,
  var = this.var ,
  ###  "pareto" (means power law) or "exponential"
  typefct = "pareto",
  # distance where the density of probability of the spatial interaction function equals 0.5.
  span = 5000,
  # impedance factor for the spatial interaction function.
  beta = 8, 
  nclass = 5,
  col = carto.pal(pal1 = 'brown.pal', n1 = 6),
  border = "grey",
  lwd = 0.1,
  mask = abacopoly,
  #legend.values.rnd = -3,
  legend.title.txt = "Severity\nIndex",
  legend.pos = "topright"
)
# annotation on the map
text(x = 692582, y = 1611478, cex = 0.8, adj = 0, font = 3,  labels =
       "Distance function:\n- type = pareto\n- beta = 8\n- span = 5 km")
# Layout
layoutLayer(title = paste0("Protection Index Spread - based on ",this.label ),
            author = "Protection Working Group, Abaco - The Bahamas, November 2019",
            sources = "Source: Key Informant Interview - HDX/DTM/IOM",
            tabtitle = TRUE,
            frame = FALSE ,
            bg = "#aad3df",
            scale = "auto")
# North arrow
north(pos = "topleft")
  


```



# Geometric aggregation

## Severity Index Map


```{r, message=FALSE, echo = FALSE, warning=FALSE}

#for (h in 1:nrow(All.method2)) {
  h <- 5
  this.var <- as.character(All.method2[ h, c("var")])
  this.label <- as.character(All.method2[ h, c("label")])
  # get Map Background
  tilesLayer(x = abaco.osm)
  #Plot symbols with choropleth coloration
  propSymbolsChoroLayer(
    spdf = abaco,
    var = "D_102_inds",
    inches = 0.25, ## size of the biggest symbol (radius for circles, width for squares, height for bars) in inches.
    border = "grey50",
    lwd = 1, #width of symbols borders.
    legend.var.pos = "topright",
    legend.var.title.txt = "Population Size\n(# of Individual)",
    var2 = this.var ,
    #classification method; one of "sd", "equal", "quantile", "fisher-jenks","q6", "geom", "arith", "em" or "msd" 
    method = "quantile",  
    nclass = 5,
    col = carto.pal(pal1 = "sand.pal", n1 = 6),
    #legend.var2.values.rnd = -2,
    legend.var2.pos = "left",
    legend.var2.title.txt = "Index Value\n",
    legend.var.style = "e"
  )
  # Layout
  layoutLayer(title = paste0("Protection Index - based on ",this.label ),
              author = "Protection Working Group, Abaco - The Bahamas, November 2019",
              sources = "Source: Key Informant Interview - HDX/DTM/IOM", 
              tabtitle = TRUE, 
              frame = FALSE ,
              bg = "#aad3df",
              scale = "auto")
  # North arrow
  north(pos = "topleft")
  
```


---


## Now getting smooth map

```{r, message=FALSE, echo = FALSE, warning=FALSE}  
  
 smoothLayer(
  spdf = abacopoly3,
  var = this.var ,
  ###  "pareto" (means power law) or "exponential"
  typefct = "pareto",
  # distance where the density of probability of the spatial interaction function equals 0.5.
  span = 5000,
  # impedance factor for the spatial interaction function.
  beta = 8, 
  nclass = 5,
  col = carto.pal(pal1 = 'brown.pal', n1 = 6),
  border = "grey",
  lwd = 0.1,
  mask = abacopoly,
  #legend.values.rnd = -3,
  legend.title.txt = "Severity\nIndex",
  legend.pos = "topright"
)
# annotation on the map
text(x = 692582, y = 1611478, cex = 0.8, adj = 0, font = 3,  labels =
       "Distance function:\n- type = pareto\n- beta = 8\n- span = 5 km")
# Layout
layoutLayer(title = paste0("Protection Index Spread - based on ",this.label ),
            author = "Protection Working Group, Abaco - The Bahamas, November 2019",
            sources = "Source: Key Informant Interview - HDX/DTM/IOM",
            tabtitle = TRUE,
            frame = FALSE ,
            bg = "#aad3df",
            scale = "auto")
# North arrow
north(pos = "topleft")
  


```

# Mean-Min Function

## Severity Index Map


```{r, message=FALSE, echo = FALSE, warning=FALSE}

#for (h in 1:nrow(All.method2)) {
  h <- 6
  this.var <- as.character(All.method2[ h, c("var")])
  this.label <- as.character(All.method2[ h, c("label")])
  # get Map Background
  tilesLayer(x = abaco.osm)
  #Plot symbols with choropleth coloration
  propSymbolsChoroLayer(
    spdf = abaco,
    var = "D_102_inds",
    inches = 0.25, ## size of the biggest symbol (radius for circles, width for squares, height for bars) in inches.
    border = "grey50",
    lwd = 1, #width of symbols borders.
    legend.var.pos = "topright",
    legend.var.title.txt = "Population Size\n(# of Individual)",
    var2 = this.var ,
    #classification method; one of "sd", "equal", "quantile", "fisher-jenks","q6", "geom", "arith", "em" or "msd" 
    method = "quantile",  
    nclass = 5,
    col = carto.pal(pal1 = "sand.pal", n1 = 6),
    #legend.var2.values.rnd = -2,
    legend.var2.pos = "left",
    legend.var2.title.txt = "Index Value\n",
    legend.var.style = "e"
  )
  # Layout
  layoutLayer(title = paste0("Protection Index - based on ",this.label ),
              author = "Protection Working Group, Abaco - The Bahamas, November 2019",
              sources = "Source: Key Informant Interview - HDX/DTM/IOM", 
              tabtitle = TRUE, 
              frame = FALSE ,
              bg = "#aad3df",
              scale = "auto")
  # North arrow
  north(pos = "topleft")
  
```



---


## Now getting smooth map

```{r, message=FALSE, echo = FALSE, warning=FALSE}  
smoothLayer(
  spdf = abacopoly3,
  var = this.var ,
  ###  "pareto" (means power law) or "exponential"
  typefct = "pareto",
  # distance where the density of probability of the spatial interaction function equals 0.5.
  span = 5000,
  # impedance factor for the spatial interaction function.
  beta = 8, 
  nclass = 5,
  col = carto.pal(pal1 = 'brown.pal', n1 = 6),
  border = "grey",
  lwd = 0.1,
  mask = abacopoly,
  #legend.values.rnd = -3,
  legend.title.txt = "Severity\nIndex",
  legend.pos = "topright"
)
# annotation on the map
text(x = 692582, y = 1611478, cex = 0.8, adj = 0, font = 3,  labels =
       "Distance function:\n- type = pareto\n- beta = 8\n- span = 5 km")
# Layout
layoutLayer(title = paste0("Protection Index Spread - based on ",this.label ),
            author = "Protection Working Group, Abaco - The Bahamas, November 2019",
            sources = "Source: Key Informant Interview - HDX/DTM/IOM",
            tabtitle = TRUE,
            frame = FALSE ,
            bg = "#aad3df",
            scale = "auto")
# North arrow
north(pos = "topleft")
  


```

# Mazziotta-Pareto Index

## Severity Index Map


```{r, message=FALSE, echo = FALSE, warning=FALSE}

#for (h in 1:nrow(All.method2)) {
  h <- 7
  this.var <- as.character(All.method2[ h, c("var")])
  this.label <- as.character(All.method2[ h, c("label")])
  # get Map Background
  tilesLayer(x = abaco.osm)
  #Plot symbols with choropleth coloration
  propSymbolsChoroLayer(
    spdf = abaco,
    var = "D_102_inds",
    inches = 0.25, ## size of the biggest symbol (radius for circles, width for squares, height for bars) in inches.
    border = "grey50",
    lwd = 1, #width of symbols borders.
    legend.var.pos = "topright",
    legend.var.title.txt = "Population Size\n(# of Individual)",
    var2 = this.var ,
    #classification method; one of "sd", "equal", "quantile", "fisher-jenks","q6", "geom", "arith", "em" or "msd" 
    method = "quantile",  
    nclass = 5,
    col = carto.pal(pal1 = "sand.pal", n1 = 6),
    #legend.var2.values.rnd = -2,
    legend.var2.pos = "left",
    legend.var2.title.txt = "Index Value\n",
    legend.var.style = "e"
  )
  # Layout
  layoutLayer(title = paste0("Protection Index - based on ",this.label ),
              author = "Protection Working Group, Abaco - The Bahamas, November 2019",
              sources = "Source: Key Informant Interview - HDX/DTM/IOM", 
              tabtitle = TRUE, 
              frame = FALSE ,
              bg = "#aad3df",
              scale = "auto")
  # North arrow
  north(pos = "topleft")
  
```


---


## Now getting smooth map

```{r, message=FALSE, echo = FALSE, warning=FALSE}  
  
smoothLayer(
  spdf = abacopoly3,
  var = this.var ,
  ###  "pareto" (means power law) or "exponential"
  typefct = "pareto",
  # distance where the density of probability of the spatial interaction function equals 0.5.
  span = 5000,
  # impedance factor for the spatial interaction function.
  beta = 8, 
  nclass = 5,
  col = carto.pal(pal1 = 'brown.pal', n1 = 6),
  border = "grey",
  lwd = 0.1,
  mask = abacopoly,
  #legend.values.rnd = -3,
  legend.title.txt = "Severity\nIndex",
  legend.pos = "topright"
)
# annotation on the map
text(x = 692582, y = 1611478, cex = 0.8, adj = 0, font = 3,  labels =
       "Distance function:\n- type = pareto\n- beta = 8\n- span = 5 km")
# Layout
layoutLayer(title = paste0("Protection Index Spread - based on ",this.label ),
            author = "Protection Working Group, Abaco - The Bahamas, November 2019",
            sources = "Source: Key Informant Interview - HDX/DTM/IOM",
            tabtitle = TRUE,
            frame = FALSE ,
            bg = "#aad3df",
            scale = "auto")
# North arrow
north(pos = "topleft")
  


```

# Wroclaw taxonomy method"

## Severity Index Map


```{r, message=FALSE, echo = FALSE, warning=FALSE}

#for (h in 1:nrow(All.method2)) {
  h <- 8
  this.var <- as.character(All.method2[ h, c("var")])
  this.label <- as.character(All.method2[ h, c("label")])
  # get Map Background
  tilesLayer(x = abaco.osm)
  #Plot symbols with choropleth coloration
  propSymbolsChoroLayer(
    spdf = abaco,
    var = "D_102_inds",
    inches = 0.25, ## size of the biggest symbol (radius for circles, width for squares, height for bars) in inches.
    border = "grey50",
    lwd = 1, #width of symbols borders.
    legend.var.pos = "topright",
    legend.var.title.txt = "Population Size\n(# of Individual)",
    var2 = this.var ,
    #classification method; one of "sd", "equal", "quantile", "fisher-jenks","q6", "geom", "arith", "em" or "msd" 
    method = "quantile",  
    nclass = 5,
    col = carto.pal(pal1 = "sand.pal", n1 = 6),
    #legend.var2.values.rnd = -2,
    legend.var2.pos = "left",
    legend.var2.title.txt = "Index Value\n",
    legend.var.style = "e"
  )
  # Layout
  layoutLayer(title = paste0("Protection Index - based on ",this.label ),
              author = "Protection Working Group, Abaco - The Bahamas, November 2019",
              sources = "Source: Key Informant Interview - HDX/DTM/IOM", 
              tabtitle = TRUE, 
              frame = FALSE ,
              bg = "#aad3df",
              scale = "auto")
  # North arrow
  north(pos = "topleft")
  
```


---


## Now getting smooth map

```{r, message=FALSE, echo = FALSE, warning=FALSE}  
  
 smoothLayer(
  spdf = abacopoly3,
  var = this.var ,
  ###  "pareto" (means power law) or "exponential"
  typefct = "pareto",
  # distance where the density of probability of the spatial interaction function equals 0.5.
  span = 5000,
  # impedance factor for the spatial interaction function.
  beta = 8, 
  nclass = 5,
  col = carto.pal(pal1 = 'brown.pal', n1 = 6),
  border = "grey",
  lwd = 0.1,
  mask = abacopoly,
  #legend.values.rnd = -3,
  legend.title.txt = "Severity\nIndex",
  legend.pos = "topright"
)
# annotation on the map
text(x = 692582, y = 1611478, cex = 0.8, adj = 0, font = 3,  labels =
       "Distance function:\n- type = pareto\n- beta = 8\n- span = 5 km")
# Layout
layoutLayer(title = paste0("Protection Index Spread - based on ",this.label ),
            author = "Protection Working Group, Abaco - The Bahamas, November 2019",
            sources = "Source: Key Informant Interview - HDX/DTM/IOM",
            tabtitle = TRUE,
            frame = FALSE ,
            bg = "#aad3df",
            scale = "auto")
# North arrow
north(pos = "topleft")
  


```
